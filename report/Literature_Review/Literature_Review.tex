\graphicspath{{Literature\_Review/fig}}
\chapter{Literature Review/Study/Related work}
\label{chap:Literature_Review}

\section{Introduction}
Reinforcement learning is a method of learning what to do by linking states to actions with a numerical reward incurred for every state-action pair. The final goal of RL is to find a path of states and actions with the maximum amount of reward. \cite{sutton_barto}

\section{Solving sliding puzzle using value iteration}
In this section a few terms will be used which are fully described in the theory chapters \ref{chap:MDP_and_DP} and \ref{chap:RL}. These terms include value iteration and policy iteration.

A 4x4 puzzle solution is investigated in \cite{15-puzzle_value_iteration}.
The method they use is a variation of value iteration. The amount of states in a 4x4 puzzle is approximately 10\^13 elements.
It was found in \cite{15-puzzle_value_iteration} that value iteration is indeed a feasible approach if the value iteration algorithm is altered so that the only a subset of the entire state space is used sequentially until a solution is found. 

This reducing of the state space is similar to what will later be done in this report. However the method we use to reduce the state space is by having our algorithm be guided somewhat by how a human would solve a puzzle, by doing the edges first. Additionally that we use a different reinforcement learning method known as policy iteration.


Solutions to sliding puzzles can also been found by using search algorithms such as A* and IDA*
\cite{search_alg}.
