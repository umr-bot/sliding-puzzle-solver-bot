\graphicspath{{RL\_theory/fig}}

\chapter{Reinforcement Learning Theory}
\label{chap:RL_theory}

Reinforcement learning is a method of learning what to do by linking states to actions with a numerical reward incurred for every state-action pair. The final goal of RL is to find a path of states and actions with the maximum amount of reward. \cite{sutton_barto}

To model reinforcement learning problems we use dynamic systems theory, specifically using incompletely-known MDP's (Markov decision
processes). Most of RL problems can be described using MDP's. In Mathematics a MDP is a stochastic, discrete time control process. What that means is that the process is essentially partially controlled and partially random. MDP's depend mainly on a few variables which are, states, actions, state transition probability ,reward and a discount factor. These variables can be denoted in a 5-tuple as:
\[(S,A,P^{a}_{ss'},R^{a}_{s},\gamma)\] where
\begin{itemize}
	\item S is a finite set of states
	\item A is a finite set of actions
	\item $P^{a}_{ss'}$ is a matrix of probabilities with $P^{a}_{ss'} = P(S_{t+1} = s' | S_t = s,A_t = a) $
	\item $R^{a}_{s}$ is the immediate reward after transitioning from state s to s' using action a. Where $R^{a}_{s} = E[R_{t+1}|S_t =s, A_t =a]$
	\item $\gamma$ $\in$ [0,1] is the discount factor applied to the reward
\end{itemize}

For a state to be Markov it needs to satisfy the following condition:
\[P[S_{t+1}|S_{t}] = P[S_{t+1}|S_{1},...,S_{t}]\]

This means that the current state is required to contain all the information of the previous states. For a MDP all states must be Markov.

We now define the definition which is the total discounted reward from time-step t, named the return $G_t$ where:
\begin{align}
	G_t &= R_{t+1}+\gamma R_{t+2} + ... \\
		&= \sum_{k=0}^{\infty}\gamma^{k} R_{t+k+1}
\end{align}

The discount $\gamma$ $\in$ [0,1] determines the present value of future rewards. For $\gamma=0$ the return $G_t$ only depends on the current reward that can be obtained in the next step $R_{t+1}$. Which can be said to be "short-sighted". While for $\gamma=1$ the return depends on all the rewards that are projected to be obtained until the process terminates. This can be said to be "far-sighted". The larger $\gamma$ is the more the rewards of later steps closer to the terminating state affects the return.

We now define another important required definition, namely the state value function v(s) where:
\[v(s) = E[G_t | S_t = s]\]
We can also decompose v(s) so that it becomes a recursive function as follows:
\begin{align}
	v(s) &= E[G_t | S_t = s]\\
		 &= E[R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ...|S_t = s]\\
		 &= E[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + ...)|S_t = s]\\
		 &= E[R_{t+1} + \gamma G_{t+1}|S_t = s]\\
		 &= E[R_{t+1} + \gamma v(S_{t+1})|S_t = s]
\end{align}